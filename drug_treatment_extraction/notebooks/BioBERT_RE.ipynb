{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioBERT RE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo/gLr2/5T2LtdOLk+d47J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacmg/task-vt/blob/biobert_finetune/drug_treatment_extraction/notebooks/BioBERT_RE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kME312A5W-O",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning BioBERT for RE\n",
        "This is a fine-tuning notebook that we used to finetune BioBERT for relation classification (on our own data, GAD and Euadr) and then convert the resulting model checkpoint to PyTorch HuggingFace library for model inference. This was done for the vaccine and therapeutics task in order to identify drug treatment relations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsaGMR9T5GC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aa90c9d6-61b9-4880-8f56-cae7cb4c7b75"
      },
      "source": [
        "!git clone https://github.com/dmis-lab/biobert \n",
        "from google.colab import auth\n",
        "from datetime import datetime\n",
        "auth.authenticate_user()\n",
        "!pip install tensorflow==1.15\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itI1GQG79YJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('biobert')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjUD_i1eqeHg",
        "colab_type": "text"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU4N5Gln9wkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./download.sh\n",
        "!fileid=\"1GJpGjQj6aZPV-EfbiQELpBkvlGtoKiyA\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GJpGjQj6aZPV-EfbiQELpBkvlGtoKiyA' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GJpGjQj6aZPV-EfbiQELpBkvlGtoKiyA\" -O biobert_w.tar.gz && rm -rf /tmp/cookies.txt\n",
        "!tar -xvf biobert_w.tar.gz\n",
        "%set_env RE_DIR datasets/RE/GAD/1\n",
        "%set_env TASK_NAME=gad\n",
        "%set_env OUTPUT_DIR=./re_outputs_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzWkYmui9_q_",
        "colab_type": "code",
        "outputId": "92cfaebc-62f4-4c44-d490-31b94bf587e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%set_env BIOBERT_DIR=biobert_large"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: BIOBERT_DIR=biobert_large\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIiTAFNhGSfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "37917e1f-f137-4fac-915e-49c64724021c"
      },
      "source": [
        "!python run_re.py --task_name=$TASK_NAME --do_train=true --do_eval=true --do_predict=true --vocab_file=$BIOBERT_DIR/vocab_cased_pubmed_pmc_30k.txt --bert_config_file=$BIOBERT_DIR/bert_config_bio_58k_large.json --init_checkpoint=$BIOBERT_DIR/bio_bert_large_1000k.ckpt.index --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --do_lower_case=false --data_dir=$RE_DIR --output_dir=$OUTPUT_DIR"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-08 17:53:02.882983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"run_re.py\", line 25, in <module>\n",
            "    import optimization\n",
            "  File \"/content/biobert/optimization.py\", line 87, in <module>\n",
            "    class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
            "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml6_KhzOHswS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uncomment this if you want to temporarily stash weights on GCS also collect garbage\n",
        "#!gsutil -m cp -r ./re_outputs_1/model.ckpt-0.data-00000-of-00001 gs://coronaviruspublicdata/new_data .\n",
        "#import gc \n",
        "#gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z57c_BNXrZTI",
        "colab_type": "text"
      },
      "source": [
        "### Converting the model to HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF9AGQO61Z2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "import logging\n",
        "import torch\n",
        "logger = logging.getLogger('spam_application')\n",
        "\n",
        "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    excluded = ['BERTAdam','_power','global_step']\n",
        "    init_vars = list(filter(lambda x:all([True if e not in x[0] else False for e in excluded]),init_vars))\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "    print(\"A name\",names)\n",
        "    for name, array in zip(names, arrays):\n",
        "        if name in ['output_weights', 'output_bias']:\n",
        "          name = 'classifier/' + name\n",
        "        name = name.split(\"/\")\n",
        "        # if name in ['output_weights', 'output_bias']:\n",
        "        #   name = 'classifier/' + name\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ):\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        pointer = model\n",
        "        # if name in ['output_weights' , 'output_bias']:\n",
        "        #   name = 'classifier/' + name\n",
        "        for m_name in name:\n",
        "\n",
        "            print(\"model\",m_name)\n",
        "            #print(scope_names)\n",
        "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
        "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
        "                print(scope_names)\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
        "            # elif scope_names[0] == \"beta\":\n",
        "            #     print(scope_names)\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "            # elif scope_names[0] == \"output_bias\":\n",
        "            #     print(scope_names)\n",
        "            #   pointer = getattr(pointer, \"cls\")\n",
        "            elif scope_names[0] == \"output_weights\":\n",
        "                print(scope_names)\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"squad\":\n",
        "                print(scope_names)\n",
        "                pointer = getattr(pointer, \"classifier\")\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, scope_names[0])\n",
        "                except AttributeError:\n",
        "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "                    continue\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == \"_embeddings\":\n",
        "            pointer = getattr(pointer, \"weight\")\n",
        "        elif m_name == \"kernel\":\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            assert pointer.shape == array.shape\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21-4zfFQ2rgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "cf6170d6-dd67-4aea-8ca6-d6c277f116b1"
      },
      "source": [
        "from transformers import BertConfig, BertForSequenceClassification, BertForPreTraining\n",
        "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
        "    # Initialise PyTorch model\n",
        "    config = BertConfig.from_json_file(bert_config_file)\n",
        "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "    config.num_labels = 2\n",
        "    model = BertForSequenceClassification(config)\n",
        "    #model = BertForSequenceClassification(config)\n",
        "    # Load \"weights from tf checkpoint\n",
        "    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
        "\n",
        "    # Save pytorch-model\n",
        "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
        "    model.save_pretrained(pytorch_dump_path)\n",
        "    return model\n",
        "# Alternatevely you can download existing stashed data\n",
        "#!gsutil cp -r gs://coronaviruspublicdata/re_outputs_1 ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20BVxb5tLXtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76dad74d-74ce-4b80-9098-981565a51b5a"
      },
      "source": [
        "\n",
        "import os\n",
        "!mkdir pytorch_output_temp\n",
        "model2 = convert_tf_checkpoint_to_pytorch(\"re_outputs_1\", \"biobert_large/bert_config_bio_58k_large.json\", \"pytorch_output_temp\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": null,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 58996\n",
            "}\n",
            "\n",
            "A name ['bert/embeddings/LayerNorm/beta', 'bert/embeddings/LayerNorm/beta/adam_m', 'bert/embeddings/LayerNorm/beta/adam_v', 'bert/embeddings/LayerNorm/gamma', 'bert/embeddings/LayerNorm/gamma/adam_m', 'bert/embeddings/LayerNorm/gamma/adam_v', 'bert/embeddings/position_embeddings', 'bert/embeddings/position_embeddings/adam_m', 'bert/embeddings/position_embeddings/adam_v', 'bert/embeddings/token_type_embeddings', 'bert/embeddings/token_type_embeddings/adam_m', 'bert/embeddings/token_type_embeddings/adam_v', 'bert/embeddings/word_embeddings', 'bert/embeddings/word_embeddings/adam_m', 'bert/embeddings/word_embeddings/adam_v', 'bert/encoder/layer_0/attention/output/LayerNorm/beta', 'bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_0/attention/output/LayerNorm/gamma', 'bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_0/attention/output/dense/bias', 'bert/encoder/layer_0/attention/output/dense/bias/adam_m', 'bert/encoder/layer_0/attention/output/dense/bias/adam_v', 'bert/encoder/layer_0/attention/output/dense/kernel', 'bert/encoder/layer_0/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_0/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_0/attention/self/key/bias', 'bert/encoder/layer_0/attention/self/key/bias/adam_m', 'bert/encoder/layer_0/attention/self/key/bias/adam_v', 'bert/encoder/layer_0/attention/self/key/kernel', 'bert/encoder/layer_0/attention/self/key/kernel/adam_m', 'bert/encoder/layer_0/attention/self/key/kernel/adam_v', 'bert/encoder/layer_0/attention/self/query/bias', 'bert/encoder/layer_0/attention/self/query/bias/adam_m', 'bert/encoder/layer_0/attention/self/query/bias/adam_v', 'bert/encoder/layer_0/attention/self/query/kernel', 'bert/encoder/layer_0/attention/self/query/kernel/adam_m', 'bert/encoder/layer_0/attention/self/query/kernel/adam_v', 'bert/encoder/layer_0/attention/self/value/bias', 'bert/encoder/layer_0/attention/self/value/bias/adam_m', 'bert/encoder/layer_0/attention/self/value/bias/adam_v', 'bert/encoder/layer_0/attention/self/value/kernel', 'bert/encoder/layer_0/attention/self/value/kernel/adam_m', 'bert/encoder/layer_0/attention/self/value/kernel/adam_v', 'bert/encoder/layer_0/intermediate/dense/bias', 'bert/encoder/layer_0/intermediate/dense/bias/adam_m', 'bert/encoder/layer_0/intermediate/dense/bias/adam_v', 'bert/encoder/layer_0/intermediate/dense/kernel', 'bert/encoder/layer_0/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_0/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_0/output/LayerNorm/beta', 'bert/encoder/layer_0/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_0/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_0/output/LayerNorm/gamma', 'bert/encoder/layer_0/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_0/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_0/output/dense/bias', 'bert/encoder/layer_0/output/dense/bias/adam_m', 'bert/encoder/layer_0/output/dense/bias/adam_v', 'bert/encoder/layer_0/output/dense/kernel', 'bert/encoder/layer_0/output/dense/kernel/adam_m', 'bert/encoder/layer_0/output/dense/kernel/adam_v', 'bert/encoder/layer_1/attention/output/LayerNorm/beta', 'bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_1/attention/output/LayerNorm/gamma', 'bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_1/attention/output/dense/bias', 'bert/encoder/layer_1/attention/output/dense/bias/adam_m', 'bert/encoder/layer_1/attention/output/dense/bias/adam_v', 'bert/encoder/layer_1/attention/output/dense/kernel', 'bert/encoder/layer_1/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_1/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_1/attention/self/key/bias', 'bert/encoder/layer_1/attention/self/key/bias/adam_m', 'bert/encoder/layer_1/attention/self/key/bias/adam_v', 'bert/encoder/layer_1/attention/self/key/kernel', 'bert/encoder/layer_1/attention/self/key/kernel/adam_m', 'bert/encoder/layer_1/attention/self/key/kernel/adam_v', 'bert/encoder/layer_1/attention/self/query/bias', 'bert/encoder/layer_1/attention/self/query/bias/adam_m', 'bert/encoder/layer_1/attention/self/query/bias/adam_v', 'bert/encoder/layer_1/attention/self/query/kernel', 'bert/encoder/layer_1/attention/self/query/kernel/adam_m', 'bert/encoder/layer_1/attention/self/query/kernel/adam_v', 'bert/encoder/layer_1/attention/self/value/bias', 'bert/encoder/layer_1/attention/self/value/bias/adam_m', 'bert/encoder/layer_1/attention/self/value/bias/adam_v', 'bert/encoder/layer_1/attention/self/value/kernel', 'bert/encoder/layer_1/attention/self/value/kernel/adam_m', 'bert/encoder/layer_1/attention/self/value/kernel/adam_v', 'bert/encoder/layer_1/intermediate/dense/bias', 'bert/encoder/layer_1/intermediate/dense/bias/adam_m', 'bert/encoder/layer_1/intermediate/dense/bias/adam_v', 'bert/encoder/layer_1/intermediate/dense/kernel', 'bert/encoder/layer_1/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_1/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_1/output/LayerNorm/beta', 'bert/encoder/layer_1/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_1/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_1/output/LayerNorm/gamma', 'bert/encoder/layer_1/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_1/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_1/output/dense/bias', 'bert/encoder/layer_1/output/dense/bias/adam_m', 'bert/encoder/layer_1/output/dense/bias/adam_v', 'bert/encoder/layer_1/output/dense/kernel', 'bert/encoder/layer_1/output/dense/kernel/adam_m', 'bert/encoder/layer_1/output/dense/kernel/adam_v', 'bert/encoder/layer_10/attention/output/LayerNorm/beta', 'bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_10/attention/output/LayerNorm/gamma', 'bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_10/attention/output/dense/bias', 'bert/encoder/layer_10/attention/output/dense/bias/adam_m', 'bert/encoder/layer_10/attention/output/dense/bias/adam_v', 'bert/encoder/layer_10/attention/output/dense/kernel', 'bert/encoder/layer_10/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_10/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_10/attention/self/key/bias', 'bert/encoder/layer_10/attention/self/key/bias/adam_m', 'bert/encoder/layer_10/attention/self/key/bias/adam_v', 'bert/encoder/layer_10/attention/self/key/kernel', 'bert/encoder/layer_10/attention/self/key/kernel/adam_m', 'bert/encoder/layer_10/attention/self/key/kernel/adam_v', 'bert/encoder/layer_10/attention/self/query/bias', 'bert/encoder/layer_10/attention/self/query/bias/adam_m', 'bert/encoder/layer_10/attention/self/query/bias/adam_v', 'bert/encoder/layer_10/attention/self/query/kernel', 'bert/encoder/layer_10/attention/self/query/kernel/adam_m', 'bert/encoder/layer_10/attention/self/query/kernel/adam_v', 'bert/encoder/layer_10/attention/self/value/bias', 'bert/encoder/layer_10/attention/self/value/bias/adam_m', 'bert/encoder/layer_10/attention/self/value/bias/adam_v', 'bert/encoder/layer_10/attention/self/value/kernel', 'bert/encoder/layer_10/attention/self/value/kernel/adam_m', 'bert/encoder/layer_10/attention/self/value/kernel/adam_v', 'bert/encoder/layer_10/intermediate/dense/bias', 'bert/encoder/layer_10/intermediate/dense/bias/adam_m', 'bert/encoder/layer_10/intermediate/dense/bias/adam_v', 'bert/encoder/layer_10/intermediate/dense/kernel', 'bert/encoder/layer_10/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_10/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_10/output/LayerNorm/beta', 'bert/encoder/layer_10/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_10/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_10/output/LayerNorm/gamma', 'bert/encoder/layer_10/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_10/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_10/output/dense/bias', 'bert/encoder/layer_10/output/dense/bias/adam_m', 'bert/encoder/layer_10/output/dense/bias/adam_v', 'bert/encoder/layer_10/output/dense/kernel', 'bert/encoder/layer_10/output/dense/kernel/adam_m', 'bert/encoder/layer_10/output/dense/kernel/adam_v', 'bert/encoder/layer_11/attention/output/LayerNorm/beta', 'bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_11/attention/output/LayerNorm/gamma', 'bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_11/attention/output/dense/bias', 'bert/encoder/layer_11/attention/output/dense/bias/adam_m', 'bert/encoder/layer_11/attention/output/dense/bias/adam_v', 'bert/encoder/layer_11/attention/output/dense/kernel', 'bert/encoder/layer_11/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_11/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_11/attention/self/key/bias', 'bert/encoder/layer_11/attention/self/key/bias/adam_m', 'bert/encoder/layer_11/attention/self/key/bias/adam_v', 'bert/encoder/layer_11/attention/self/key/kernel', 'bert/encoder/layer_11/attention/self/key/kernel/adam_m', 'bert/encoder/layer_11/attention/self/key/kernel/adam_v', 'bert/encoder/layer_11/attention/self/query/bias', 'bert/encoder/layer_11/attention/self/query/bias/adam_m', 'bert/encoder/layer_11/attention/self/query/bias/adam_v', 'bert/encoder/layer_11/attention/self/query/kernel', 'bert/encoder/layer_11/attention/self/query/kernel/adam_m', 'bert/encoder/layer_11/attention/self/query/kernel/adam_v', 'bert/encoder/layer_11/attention/self/value/bias', 'bert/encoder/layer_11/attention/self/value/bias/adam_m', 'bert/encoder/layer_11/attention/self/value/bias/adam_v', 'bert/encoder/layer_11/attention/self/value/kernel', 'bert/encoder/layer_11/attention/self/value/kernel/adam_m', 'bert/encoder/layer_11/attention/self/value/kernel/adam_v', 'bert/encoder/layer_11/intermediate/dense/bias', 'bert/encoder/layer_11/intermediate/dense/bias/adam_m', 'bert/encoder/layer_11/intermediate/dense/bias/adam_v', 'bert/encoder/layer_11/intermediate/dense/kernel', 'bert/encoder/layer_11/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_11/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_11/output/LayerNorm/beta', 'bert/encoder/layer_11/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_11/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_11/output/LayerNorm/gamma', 'bert/encoder/layer_11/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_11/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_11/output/dense/bias', 'bert/encoder/layer_11/output/dense/bias/adam_m', 'bert/encoder/layer_11/output/dense/bias/adam_v', 'bert/encoder/layer_11/output/dense/kernel', 'bert/encoder/layer_11/output/dense/kernel/adam_m', 'bert/encoder/layer_11/output/dense/kernel/adam_v', 'bert/encoder/layer_12/attention/output/LayerNorm/beta', 'bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_12/attention/output/LayerNorm/gamma', 'bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_12/attention/output/dense/bias', 'bert/encoder/layer_12/attention/output/dense/bias/adam_m', 'bert/encoder/layer_12/attention/output/dense/bias/adam_v', 'bert/encoder/layer_12/attention/output/dense/kernel', 'bert/encoder/layer_12/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_12/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_12/attention/self/key/bias', 'bert/encoder/layer_12/attention/self/key/bias/adam_m', 'bert/encoder/layer_12/attention/self/key/bias/adam_v', 'bert/encoder/layer_12/attention/self/key/kernel', 'bert/encoder/layer_12/attention/self/key/kernel/adam_m', 'bert/encoder/layer_12/attention/self/key/kernel/adam_v', 'bert/encoder/layer_12/attention/self/query/bias', 'bert/encoder/layer_12/attention/self/query/bias/adam_m', 'bert/encoder/layer_12/attention/self/query/bias/adam_v', 'bert/encoder/layer_12/attention/self/query/kernel', 'bert/encoder/layer_12/attention/self/query/kernel/adam_m', 'bert/encoder/layer_12/attention/self/query/kernel/adam_v', 'bert/encoder/layer_12/attention/self/value/bias', 'bert/encoder/layer_12/attention/self/value/bias/adam_m', 'bert/encoder/layer_12/attention/self/value/bias/adam_v', 'bert/encoder/layer_12/attention/self/value/kernel', 'bert/encoder/layer_12/attention/self/value/kernel/adam_m', 'bert/encoder/layer_12/attention/self/value/kernel/adam_v', 'bert/encoder/layer_12/intermediate/dense/bias', 'bert/encoder/layer_12/intermediate/dense/bias/adam_m', 'bert/encoder/layer_12/intermediate/dense/bias/adam_v', 'bert/encoder/layer_12/intermediate/dense/kernel', 'bert/encoder/layer_12/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_12/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_12/output/LayerNorm/beta', 'bert/encoder/layer_12/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_12/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_12/output/LayerNorm/gamma', 'bert/encoder/layer_12/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_12/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_12/output/dense/bias', 'bert/encoder/layer_12/output/dense/bias/adam_m', 'bert/encoder/layer_12/output/dense/bias/adam_v', 'bert/encoder/layer_12/output/dense/kernel', 'bert/encoder/layer_12/output/dense/kernel/adam_m', 'bert/encoder/layer_12/output/dense/kernel/adam_v', 'bert/encoder/layer_13/attention/output/LayerNorm/beta', 'bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_13/attention/output/LayerNorm/gamma', 'bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_13/attention/output/dense/bias', 'bert/encoder/layer_13/attention/output/dense/bias/adam_m', 'bert/encoder/layer_13/attention/output/dense/bias/adam_v', 'bert/encoder/layer_13/attention/output/dense/kernel', 'bert/encoder/layer_13/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_13/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_13/attention/self/key/bias', 'bert/encoder/layer_13/attention/self/key/bias/adam_m', 'bert/encoder/layer_13/attention/self/key/bias/adam_v', 'bert/encoder/layer_13/attention/self/key/kernel', 'bert/encoder/layer_13/attention/self/key/kernel/adam_m', 'bert/encoder/layer_13/attention/self/key/kernel/adam_v', 'bert/encoder/layer_13/attention/self/query/bias', 'bert/encoder/layer_13/attention/self/query/bias/adam_m', 'bert/encoder/layer_13/attention/self/query/bias/adam_v', 'bert/encoder/layer_13/attention/self/query/kernel', 'bert/encoder/layer_13/attention/self/query/kernel/adam_m', 'bert/encoder/layer_13/attention/self/query/kernel/adam_v', 'bert/encoder/layer_13/attention/self/value/bias', 'bert/encoder/layer_13/attention/self/value/bias/adam_m', 'bert/encoder/layer_13/attention/self/value/bias/adam_v', 'bert/encoder/layer_13/attention/self/value/kernel', 'bert/encoder/layer_13/attention/self/value/kernel/adam_m', 'bert/encoder/layer_13/attention/self/value/kernel/adam_v', 'bert/encoder/layer_13/intermediate/dense/bias', 'bert/encoder/layer_13/intermediate/dense/bias/adam_m', 'bert/encoder/layer_13/intermediate/dense/bias/adam_v', 'bert/encoder/layer_13/intermediate/dense/kernel', 'bert/encoder/layer_13/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_13/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_13/output/LayerNorm/beta', 'bert/encoder/layer_13/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_13/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_13/output/LayerNorm/gamma', 'bert/encoder/layer_13/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_13/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_13/output/dense/bias', 'bert/encoder/layer_13/output/dense/bias/adam_m', 'bert/encoder/layer_13/output/dense/bias/adam_v', 'bert/encoder/layer_13/output/dense/kernel', 'bert/encoder/layer_13/output/dense/kernel/adam_m', 'bert/encoder/layer_13/output/dense/kernel/adam_v', 'bert/encoder/layer_14/attention/output/LayerNorm/beta', 'bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_14/attention/output/LayerNorm/gamma', 'bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_14/attention/output/dense/bias', 'bert/encoder/layer_14/attention/output/dense/bias/adam_m', 'bert/encoder/layer_14/attention/output/dense/bias/adam_v', 'bert/encoder/layer_14/attention/output/dense/kernel', 'bert/encoder/layer_14/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_14/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_14/attention/self/key/bias', 'bert/encoder/layer_14/attention/self/key/bias/adam_m', 'bert/encoder/layer_14/attention/self/key/bias/adam_v', 'bert/encoder/layer_14/attention/self/key/kernel', 'bert/encoder/layer_14/attention/self/key/kernel/adam_m', 'bert/encoder/layer_14/attention/self/key/kernel/adam_v', 'bert/encoder/layer_14/attention/self/query/bias', 'bert/encoder/layer_14/attention/self/query/bias/adam_m', 'bert/encoder/layer_14/attention/self/query/bias/adam_v', 'bert/encoder/layer_14/attention/self/query/kernel', 'bert/encoder/layer_14/attention/self/query/kernel/adam_m', 'bert/encoder/layer_14/attention/self/query/kernel/adam_v', 'bert/encoder/layer_14/attention/self/value/bias', 'bert/encoder/layer_14/attention/self/value/bias/adam_m', 'bert/encoder/layer_14/attention/self/value/bias/adam_v', 'bert/encoder/layer_14/attention/self/value/kernel', 'bert/encoder/layer_14/attention/self/value/kernel/adam_m', 'bert/encoder/layer_14/attention/self/value/kernel/adam_v', 'bert/encoder/layer_14/intermediate/dense/bias', 'bert/encoder/layer_14/intermediate/dense/bias/adam_m', 'bert/encoder/layer_14/intermediate/dense/bias/adam_v', 'bert/encoder/layer_14/intermediate/dense/kernel', 'bert/encoder/layer_14/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_14/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_14/output/LayerNorm/beta', 'bert/encoder/layer_14/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_14/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_14/output/LayerNorm/gamma', 'bert/encoder/layer_14/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_14/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_14/output/dense/bias', 'bert/encoder/layer_14/output/dense/bias/adam_m', 'bert/encoder/layer_14/output/dense/bias/adam_v', 'bert/encoder/layer_14/output/dense/kernel', 'bert/encoder/layer_14/output/dense/kernel/adam_m', 'bert/encoder/layer_14/output/dense/kernel/adam_v', 'bert/encoder/layer_15/attention/output/LayerNorm/beta', 'bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_15/attention/output/LayerNorm/gamma', 'bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_15/attention/output/dense/bias', 'bert/encoder/layer_15/attention/output/dense/bias/adam_m', 'bert/encoder/layer_15/attention/output/dense/bias/adam_v', 'bert/encoder/layer_15/attention/output/dense/kernel', 'bert/encoder/layer_15/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_15/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_15/attention/self/key/bias', 'bert/encoder/layer_15/attention/self/key/bias/adam_m', 'bert/encoder/layer_15/attention/self/key/bias/adam_v', 'bert/encoder/layer_15/attention/self/key/kernel', 'bert/encoder/layer_15/attention/self/key/kernel/adam_m', 'bert/encoder/layer_15/attention/self/key/kernel/adam_v', 'bert/encoder/layer_15/attention/self/query/bias', 'bert/encoder/layer_15/attention/self/query/bias/adam_m', 'bert/encoder/layer_15/attention/self/query/bias/adam_v', 'bert/encoder/layer_15/attention/self/query/kernel', 'bert/encoder/layer_15/attention/self/query/kernel/adam_m', 'bert/encoder/layer_15/attention/self/query/kernel/adam_v', 'bert/encoder/layer_15/attention/self/value/bias', 'bert/encoder/layer_15/attention/self/value/bias/adam_m', 'bert/encoder/layer_15/attention/self/value/bias/adam_v', 'bert/encoder/layer_15/attention/self/value/kernel', 'bert/encoder/layer_15/attention/self/value/kernel/adam_m', 'bert/encoder/layer_15/attention/self/value/kernel/adam_v', 'bert/encoder/layer_15/intermediate/dense/bias', 'bert/encoder/layer_15/intermediate/dense/bias/adam_m', 'bert/encoder/layer_15/intermediate/dense/bias/adam_v', 'bert/encoder/layer_15/intermediate/dense/kernel', 'bert/encoder/layer_15/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_15/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_15/output/LayerNorm/beta', 'bert/encoder/layer_15/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_15/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_15/output/LayerNorm/gamma', 'bert/encoder/layer_15/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_15/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_15/output/dense/bias', 'bert/encoder/layer_15/output/dense/bias/adam_m', 'bert/encoder/layer_15/output/dense/bias/adam_v', 'bert/encoder/layer_15/output/dense/kernel', 'bert/encoder/layer_15/output/dense/kernel/adam_m', 'bert/encoder/layer_15/output/dense/kernel/adam_v', 'bert/encoder/layer_16/attention/output/LayerNorm/beta', 'bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_16/attention/output/LayerNorm/gamma', 'bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_16/attention/output/dense/bias', 'bert/encoder/layer_16/attention/output/dense/bias/adam_m', 'bert/encoder/layer_16/attention/output/dense/bias/adam_v', 'bert/encoder/layer_16/attention/output/dense/kernel', 'bert/encoder/layer_16/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_16/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_16/attention/self/key/bias', 'bert/encoder/layer_16/attention/self/key/bias/adam_m', 'bert/encoder/layer_16/attention/self/key/bias/adam_v', 'bert/encoder/layer_16/attention/self/key/kernel', 'bert/encoder/layer_16/attention/self/key/kernel/adam_m', 'bert/encoder/layer_16/attention/self/key/kernel/adam_v', 'bert/encoder/layer_16/attention/self/query/bias', 'bert/encoder/layer_16/attention/self/query/bias/adam_m', 'bert/encoder/layer_16/attention/self/query/bias/adam_v', 'bert/encoder/layer_16/attention/self/query/kernel', 'bert/encoder/layer_16/attention/self/query/kernel/adam_m', 'bert/encoder/layer_16/attention/self/query/kernel/adam_v', 'bert/encoder/layer_16/attention/self/value/bias', 'bert/encoder/layer_16/attention/self/value/bias/adam_m', 'bert/encoder/layer_16/attention/self/value/bias/adam_v', 'bert/encoder/layer_16/attention/self/value/kernel', 'bert/encoder/layer_16/attention/self/value/kernel/adam_m', 'bert/encoder/layer_16/attention/self/value/kernel/adam_v', 'bert/encoder/layer_16/intermediate/dense/bias', 'bert/encoder/layer_16/intermediate/dense/bias/adam_m', 'bert/encoder/layer_16/intermediate/dense/bias/adam_v', 'bert/encoder/layer_16/intermediate/dense/kernel', 'bert/encoder/layer_16/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_16/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_16/output/LayerNorm/beta', 'bert/encoder/layer_16/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_16/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_16/output/LayerNorm/gamma', 'bert/encoder/layer_16/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_16/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_16/output/dense/bias', 'bert/encoder/layer_16/output/dense/bias/adam_m', 'bert/encoder/layer_16/output/dense/bias/adam_v', 'bert/encoder/layer_16/output/dense/kernel', 'bert/encoder/layer_16/output/dense/kernel/adam_m', 'bert/encoder/layer_16/output/dense/kernel/adam_v', 'bert/encoder/layer_17/attention/output/LayerNorm/beta', 'bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_17/attention/output/LayerNorm/gamma', 'bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_17/attention/output/dense/bias', 'bert/encoder/layer_17/attention/output/dense/bias/adam_m', 'bert/encoder/layer_17/attention/output/dense/bias/adam_v', 'bert/encoder/layer_17/attention/output/dense/kernel', 'bert/encoder/layer_17/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_17/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_17/attention/self/key/bias', 'bert/encoder/layer_17/attention/self/key/bias/adam_m', 'bert/encoder/layer_17/attention/self/key/bias/adam_v', 'bert/encoder/layer_17/attention/self/key/kernel', 'bert/encoder/layer_17/attention/self/key/kernel/adam_m', 'bert/encoder/layer_17/attention/self/key/kernel/adam_v', 'bert/encoder/layer_17/attention/self/query/bias', 'bert/encoder/layer_17/attention/self/query/bias/adam_m', 'bert/encoder/layer_17/attention/self/query/bias/adam_v', 'bert/encoder/layer_17/attention/self/query/kernel', 'bert/encoder/layer_17/attention/self/query/kernel/adam_m', 'bert/encoder/layer_17/attention/self/query/kernel/adam_v', 'bert/encoder/layer_17/attention/self/value/bias', 'bert/encoder/layer_17/attention/self/value/bias/adam_m', 'bert/encoder/layer_17/attention/self/value/bias/adam_v', 'bert/encoder/layer_17/attention/self/value/kernel', 'bert/encoder/layer_17/attention/self/value/kernel/adam_m', 'bert/encoder/layer_17/attention/self/value/kernel/adam_v', 'bert/encoder/layer_17/intermediate/dense/bias', 'bert/encoder/layer_17/intermediate/dense/bias/adam_m', 'bert/encoder/layer_17/intermediate/dense/bias/adam_v', 'bert/encoder/layer_17/intermediate/dense/kernel', 'bert/encoder/layer_17/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_17/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_17/output/LayerNorm/beta', 'bert/encoder/layer_17/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_17/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_17/output/LayerNorm/gamma', 'bert/encoder/layer_17/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_17/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_17/output/dense/bias', 'bert/encoder/layer_17/output/dense/bias/adam_m', 'bert/encoder/layer_17/output/dense/bias/adam_v', 'bert/encoder/layer_17/output/dense/kernel', 'bert/encoder/layer_17/output/dense/kernel/adam_m', 'bert/encoder/layer_17/output/dense/kernel/adam_v', 'bert/encoder/layer_18/attention/output/LayerNorm/beta', 'bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_18/attention/output/LayerNorm/gamma', 'bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_18/attention/output/dense/bias', 'bert/encoder/layer_18/attention/output/dense/bias/adam_m', 'bert/encoder/layer_18/attention/output/dense/bias/adam_v', 'bert/encoder/layer_18/attention/output/dense/kernel', 'bert/encoder/layer_18/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_18/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_18/attention/self/key/bias', 'bert/encoder/layer_18/attention/self/key/bias/adam_m', 'bert/encoder/layer_18/attention/self/key/bias/adam_v', 'bert/encoder/layer_18/attention/self/key/kernel', 'bert/encoder/layer_18/attention/self/key/kernel/adam_m', 'bert/encoder/layer_18/attention/self/key/kernel/adam_v', 'bert/encoder/layer_18/attention/self/query/bias', 'bert/encoder/layer_18/attention/self/query/bias/adam_m', 'bert/encoder/layer_18/attention/self/query/bias/adam_v', 'bert/encoder/layer_18/attention/self/query/kernel', 'bert/encoder/layer_18/attention/self/query/kernel/adam_m', 'bert/encoder/layer_18/attention/self/query/kernel/adam_v', 'bert/encoder/layer_18/attention/self/value/bias', 'bert/encoder/layer_18/attention/self/value/bias/adam_m', 'bert/encoder/layer_18/attention/self/value/bias/adam_v', 'bert/encoder/layer_18/attention/self/value/kernel', 'bert/encoder/layer_18/attention/self/value/kernel/adam_m', 'bert/encoder/layer_18/attention/self/value/kernel/adam_v', 'bert/encoder/layer_18/intermediate/dense/bias', 'bert/encoder/layer_18/intermediate/dense/bias/adam_m', 'bert/encoder/layer_18/intermediate/dense/bias/adam_v', 'bert/encoder/layer_18/intermediate/dense/kernel', 'bert/encoder/layer_18/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_18/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_18/output/LayerNorm/beta', 'bert/encoder/layer_18/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_18/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_18/output/LayerNorm/gamma', 'bert/encoder/layer_18/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_18/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_18/output/dense/bias', 'bert/encoder/layer_18/output/dense/bias/adam_m', 'bert/encoder/layer_18/output/dense/bias/adam_v', 'bert/encoder/layer_18/output/dense/kernel', 'bert/encoder/layer_18/output/dense/kernel/adam_m', 'bert/encoder/layer_18/output/dense/kernel/adam_v', 'bert/encoder/layer_19/attention/output/LayerNorm/beta', 'bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_19/attention/output/LayerNorm/gamma', 'bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_19/attention/output/dense/bias', 'bert/encoder/layer_19/attention/output/dense/bias/adam_m', 'bert/encoder/layer_19/attention/output/dense/bias/adam_v', 'bert/encoder/layer_19/attention/output/dense/kernel', 'bert/encoder/layer_19/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_19/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_19/attention/self/key/bias', 'bert/encoder/layer_19/attention/self/key/bias/adam_m', 'bert/encoder/layer_19/attention/self/key/bias/adam_v', 'bert/encoder/layer_19/attention/self/key/kernel', 'bert/encoder/layer_19/attention/self/key/kernel/adam_m', 'bert/encoder/layer_19/attention/self/key/kernel/adam_v', 'bert/encoder/layer_19/attention/self/query/bias', 'bert/encoder/layer_19/attention/self/query/bias/adam_m', 'bert/encoder/layer_19/attention/self/query/bias/adam_v', 'bert/encoder/layer_19/attention/self/query/kernel', 'bert/encoder/layer_19/attention/self/query/kernel/adam_m', 'bert/encoder/layer_19/attention/self/query/kernel/adam_v', 'bert/encoder/layer_19/attention/self/value/bias', 'bert/encoder/layer_19/attention/self/value/bias/adam_m', 'bert/encoder/layer_19/attention/self/value/bias/adam_v', 'bert/encoder/layer_19/attention/self/value/kernel', 'bert/encoder/layer_19/attention/self/value/kernel/adam_m', 'bert/encoder/layer_19/attention/self/value/kernel/adam_v', 'bert/encoder/layer_19/intermediate/dense/bias', 'bert/encoder/layer_19/intermediate/dense/bias/adam_m', 'bert/encoder/layer_19/intermediate/dense/bias/adam_v', 'bert/encoder/layer_19/intermediate/dense/kernel', 'bert/encoder/layer_19/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_19/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_19/output/LayerNorm/beta', 'bert/encoder/layer_19/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_19/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_19/output/LayerNorm/gamma', 'bert/encoder/layer_19/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_19/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_19/output/dense/bias', 'bert/encoder/layer_19/output/dense/bias/adam_m', 'bert/encoder/layer_19/output/dense/bias/adam_v', 'bert/encoder/layer_19/output/dense/kernel', 'bert/encoder/layer_19/output/dense/kernel/adam_m', 'bert/encoder/layer_19/output/dense/kernel/adam_v', 'bert/encoder/layer_2/attention/output/LayerNorm/beta', 'bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_2/attention/output/LayerNorm/gamma', 'bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_2/attention/output/dense/bias', 'bert/encoder/layer_2/attention/output/dense/bias/adam_m', 'bert/encoder/layer_2/attention/output/dense/bias/adam_v', 'bert/encoder/layer_2/attention/output/dense/kernel', 'bert/encoder/layer_2/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_2/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_2/attention/self/key/bias', 'bert/encoder/layer_2/attention/self/key/bias/adam_m', 'bert/encoder/layer_2/attention/self/key/bias/adam_v', 'bert/encoder/layer_2/attention/self/key/kernel', 'bert/encoder/layer_2/attention/self/key/kernel/adam_m', 'bert/encoder/layer_2/attention/self/key/kernel/adam_v', 'bert/encoder/layer_2/attention/self/query/bias', 'bert/encoder/layer_2/attention/self/query/bias/adam_m', 'bert/encoder/layer_2/attention/self/query/bias/adam_v', 'bert/encoder/layer_2/attention/self/query/kernel', 'bert/encoder/layer_2/attention/self/query/kernel/adam_m', 'bert/encoder/layer_2/attention/self/query/kernel/adam_v', 'bert/encoder/layer_2/attention/self/value/bias', 'bert/encoder/layer_2/attention/self/value/bias/adam_m', 'bert/encoder/layer_2/attention/self/value/bias/adam_v', 'bert/encoder/layer_2/attention/self/value/kernel', 'bert/encoder/layer_2/attention/self/value/kernel/adam_m', 'bert/encoder/layer_2/attention/self/value/kernel/adam_v', 'bert/encoder/layer_2/intermediate/dense/bias', 'bert/encoder/layer_2/intermediate/dense/bias/adam_m', 'bert/encoder/layer_2/intermediate/dense/bias/adam_v', 'bert/encoder/layer_2/intermediate/dense/kernel', 'bert/encoder/layer_2/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_2/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_2/output/LayerNorm/beta', 'bert/encoder/layer_2/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_2/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_2/output/LayerNorm/gamma', 'bert/encoder/layer_2/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_2/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_2/output/dense/bias', 'bert/encoder/layer_2/output/dense/bias/adam_m', 'bert/encoder/layer_2/output/dense/bias/adam_v', 'bert/encoder/layer_2/output/dense/kernel', 'bert/encoder/layer_2/output/dense/kernel/adam_m', 'bert/encoder/layer_2/output/dense/kernel/adam_v', 'bert/encoder/layer_20/attention/output/LayerNorm/beta', 'bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_20/attention/output/LayerNorm/gamma', 'bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_20/attention/output/dense/bias', 'bert/encoder/layer_20/attention/output/dense/bias/adam_m', 'bert/encoder/layer_20/attention/output/dense/bias/adam_v', 'bert/encoder/layer_20/attention/output/dense/kernel', 'bert/encoder/layer_20/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_20/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_20/attention/self/key/bias', 'bert/encoder/layer_20/attention/self/key/bias/adam_m', 'bert/encoder/layer_20/attention/self/key/bias/adam_v', 'bert/encoder/layer_20/attention/self/key/kernel', 'bert/encoder/layer_20/attention/self/key/kernel/adam_m', 'bert/encoder/layer_20/attention/self/key/kernel/adam_v', 'bert/encoder/layer_20/attention/self/query/bias', 'bert/encoder/layer_20/attention/self/query/bias/adam_m', 'bert/encoder/layer_20/attention/self/query/bias/adam_v', 'bert/encoder/layer_20/attention/self/query/kernel', 'bert/encoder/layer_20/attention/self/query/kernel/adam_m', 'bert/encoder/layer_20/attention/self/query/kernel/adam_v', 'bert/encoder/layer_20/attention/self/value/bias', 'bert/encoder/layer_20/attention/self/value/bias/adam_m', 'bert/encoder/layer_20/attention/self/value/bias/adam_v', 'bert/encoder/layer_20/attention/self/value/kernel', 'bert/encoder/layer_20/attention/self/value/kernel/adam_m', 'bert/encoder/layer_20/attention/self/value/kernel/adam_v', 'bert/encoder/layer_20/intermediate/dense/bias', 'bert/encoder/layer_20/intermediate/dense/bias/adam_m', 'bert/encoder/layer_20/intermediate/dense/bias/adam_v', 'bert/encoder/layer_20/intermediate/dense/kernel', 'bert/encoder/layer_20/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_20/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_20/output/LayerNorm/beta', 'bert/encoder/layer_20/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_20/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_20/output/LayerNorm/gamma', 'bert/encoder/layer_20/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_20/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_20/output/dense/bias', 'bert/encoder/layer_20/output/dense/bias/adam_m', 'bert/encoder/layer_20/output/dense/bias/adam_v', 'bert/encoder/layer_20/output/dense/kernel', 'bert/encoder/layer_20/output/dense/kernel/adam_m', 'bert/encoder/layer_20/output/dense/kernel/adam_v', 'bert/encoder/layer_21/attention/output/LayerNorm/beta', 'bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_21/attention/output/LayerNorm/gamma', 'bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_21/attention/output/dense/bias', 'bert/encoder/layer_21/attention/output/dense/bias/adam_m', 'bert/encoder/layer_21/attention/output/dense/bias/adam_v', 'bert/encoder/layer_21/attention/output/dense/kernel', 'bert/encoder/layer_21/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_21/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_21/attention/self/key/bias', 'bert/encoder/layer_21/attention/self/key/bias/adam_m', 'bert/encoder/layer_21/attention/self/key/bias/adam_v', 'bert/encoder/layer_21/attention/self/key/kernel', 'bert/encoder/layer_21/attention/self/key/kernel/adam_m', 'bert/encoder/layer_21/attention/self/key/kernel/adam_v', 'bert/encoder/layer_21/attention/self/query/bias', 'bert/encoder/layer_21/attention/self/query/bias/adam_m', 'bert/encoder/layer_21/attention/self/query/bias/adam_v', 'bert/encoder/layer_21/attention/self/query/kernel', 'bert/encoder/layer_21/attention/self/query/kernel/adam_m', 'bert/encoder/layer_21/attention/self/query/kernel/adam_v', 'bert/encoder/layer_21/attention/self/value/bias', 'bert/encoder/layer_21/attention/self/value/bias/adam_m', 'bert/encoder/layer_21/attention/self/value/bias/adam_v', 'bert/encoder/layer_21/attention/self/value/kernel', 'bert/encoder/layer_21/attention/self/value/kernel/adam_m', 'bert/encoder/layer_21/attention/self/value/kernel/adam_v', 'bert/encoder/layer_21/intermediate/dense/bias', 'bert/encoder/layer_21/intermediate/dense/bias/adam_m', 'bert/encoder/layer_21/intermediate/dense/bias/adam_v', 'bert/encoder/layer_21/intermediate/dense/kernel', 'bert/encoder/layer_21/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_21/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_21/output/LayerNorm/beta', 'bert/encoder/layer_21/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_21/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_21/output/LayerNorm/gamma', 'bert/encoder/layer_21/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_21/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_21/output/dense/bias', 'bert/encoder/layer_21/output/dense/bias/adam_m', 'bert/encoder/layer_21/output/dense/bias/adam_v', 'bert/encoder/layer_21/output/dense/kernel', 'bert/encoder/layer_21/output/dense/kernel/adam_m', 'bert/encoder/layer_21/output/dense/kernel/adam_v', 'bert/encoder/layer_22/attention/output/LayerNorm/beta', 'bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_22/attention/output/LayerNorm/gamma', 'bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_22/attention/output/dense/bias', 'bert/encoder/layer_22/attention/output/dense/bias/adam_m', 'bert/encoder/layer_22/attention/output/dense/bias/adam_v', 'bert/encoder/layer_22/attention/output/dense/kernel', 'bert/encoder/layer_22/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_22/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_22/attention/self/key/bias', 'bert/encoder/layer_22/attention/self/key/bias/adam_m', 'bert/encoder/layer_22/attention/self/key/bias/adam_v', 'bert/encoder/layer_22/attention/self/key/kernel', 'bert/encoder/layer_22/attention/self/key/kernel/adam_m', 'bert/encoder/layer_22/attention/self/key/kernel/adam_v', 'bert/encoder/layer_22/attention/self/query/bias', 'bert/encoder/layer_22/attention/self/query/bias/adam_m', 'bert/encoder/layer_22/attention/self/query/bias/adam_v', 'bert/encoder/layer_22/attention/self/query/kernel', 'bert/encoder/layer_22/attention/self/query/kernel/adam_m', 'bert/encoder/layer_22/attention/self/query/kernel/adam_v', 'bert/encoder/layer_22/attention/self/value/bias', 'bert/encoder/layer_22/attention/self/value/bias/adam_m', 'bert/encoder/layer_22/attention/self/value/bias/adam_v', 'bert/encoder/layer_22/attention/self/value/kernel', 'bert/encoder/layer_22/attention/self/value/kernel/adam_m', 'bert/encoder/layer_22/attention/self/value/kernel/adam_v', 'bert/encoder/layer_22/intermediate/dense/bias', 'bert/encoder/layer_22/intermediate/dense/bias/adam_m', 'bert/encoder/layer_22/intermediate/dense/bias/adam_v', 'bert/encoder/layer_22/intermediate/dense/kernel', 'bert/encoder/layer_22/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_22/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_22/output/LayerNorm/beta', 'bert/encoder/layer_22/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_22/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_22/output/LayerNorm/gamma', 'bert/encoder/layer_22/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_22/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_22/output/dense/bias', 'bert/encoder/layer_22/output/dense/bias/adam_m', 'bert/encoder/layer_22/output/dense/bias/adam_v', 'bert/encoder/layer_22/output/dense/kernel', 'bert/encoder/layer_22/output/dense/kernel/adam_m', 'bert/encoder/layer_22/output/dense/kernel/adam_v', 'bert/encoder/layer_23/attention/output/LayerNorm/beta', 'bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_23/attention/output/LayerNorm/gamma', 'bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_23/attention/output/dense/bias', 'bert/encoder/layer_23/attention/output/dense/bias/adam_m', 'bert/encoder/layer_23/attention/output/dense/bias/adam_v', 'bert/encoder/layer_23/attention/output/dense/kernel', 'bert/encoder/layer_23/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_23/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_23/attention/self/key/bias', 'bert/encoder/layer_23/attention/self/key/bias/adam_m', 'bert/encoder/layer_23/attention/self/key/bias/adam_v', 'bert/encoder/layer_23/attention/self/key/kernel', 'bert/encoder/layer_23/attention/self/key/kernel/adam_m', 'bert/encoder/layer_23/attention/self/key/kernel/adam_v', 'bert/encoder/layer_23/attention/self/query/bias', 'bert/encoder/layer_23/attention/self/query/bias/adam_m', 'bert/encoder/layer_23/attention/self/query/bias/adam_v', 'bert/encoder/layer_23/attention/self/query/kernel', 'bert/encoder/layer_23/attention/self/query/kernel/adam_m', 'bert/encoder/layer_23/attention/self/query/kernel/adam_v', 'bert/encoder/layer_23/attention/self/value/bias', 'bert/encoder/layer_23/attention/self/value/bias/adam_m', 'bert/encoder/layer_23/attention/self/value/bias/adam_v', 'bert/encoder/layer_23/attention/self/value/kernel', 'bert/encoder/layer_23/attention/self/value/kernel/adam_m', 'bert/encoder/layer_23/attention/self/value/kernel/adam_v', 'bert/encoder/layer_23/intermediate/dense/bias', 'bert/encoder/layer_23/intermediate/dense/bias/adam_m', 'bert/encoder/layer_23/intermediate/dense/bias/adam_v', 'bert/encoder/layer_23/intermediate/dense/kernel', 'bert/encoder/layer_23/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_23/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_23/output/LayerNorm/beta', 'bert/encoder/layer_23/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_23/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_23/output/LayerNorm/gamma', 'bert/encoder/layer_23/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_23/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_23/output/dense/bias', 'bert/encoder/layer_23/output/dense/bias/adam_m', 'bert/encoder/layer_23/output/dense/bias/adam_v', 'bert/encoder/layer_23/output/dense/kernel', 'bert/encoder/layer_23/output/dense/kernel/adam_m', 'bert/encoder/layer_23/output/dense/kernel/adam_v', 'bert/encoder/layer_3/attention/output/LayerNorm/beta', 'bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_3/attention/output/LayerNorm/gamma', 'bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_3/attention/output/dense/bias', 'bert/encoder/layer_3/attention/output/dense/bias/adam_m', 'bert/encoder/layer_3/attention/output/dense/bias/adam_v', 'bert/encoder/layer_3/attention/output/dense/kernel', 'bert/encoder/layer_3/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_3/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_3/attention/self/key/bias', 'bert/encoder/layer_3/attention/self/key/bias/adam_m', 'bert/encoder/layer_3/attention/self/key/bias/adam_v', 'bert/encoder/layer_3/attention/self/key/kernel', 'bert/encoder/layer_3/attention/self/key/kernel/adam_m', 'bert/encoder/layer_3/attention/self/key/kernel/adam_v', 'bert/encoder/layer_3/attention/self/query/bias', 'bert/encoder/layer_3/attention/self/query/bias/adam_m', 'bert/encoder/layer_3/attention/self/query/bias/adam_v', 'bert/encoder/layer_3/attention/self/query/kernel', 'bert/encoder/layer_3/attention/self/query/kernel/adam_m', 'bert/encoder/layer_3/attention/self/query/kernel/adam_v', 'bert/encoder/layer_3/attention/self/value/bias', 'bert/encoder/layer_3/attention/self/value/bias/adam_m', 'bert/encoder/layer_3/attention/self/value/bias/adam_v', 'bert/encoder/layer_3/attention/self/value/kernel', 'bert/encoder/layer_3/attention/self/value/kernel/adam_m', 'bert/encoder/layer_3/attention/self/value/kernel/adam_v', 'bert/encoder/layer_3/intermediate/dense/bias', 'bert/encoder/layer_3/intermediate/dense/bias/adam_m', 'bert/encoder/layer_3/intermediate/dense/bias/adam_v', 'bert/encoder/layer_3/intermediate/dense/kernel', 'bert/encoder/layer_3/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_3/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_3/output/LayerNorm/beta', 'bert/encoder/layer_3/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_3/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_3/output/LayerNorm/gamma', 'bert/encoder/layer_3/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_3/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_3/output/dense/bias', 'bert/encoder/layer_3/output/dense/bias/adam_m', 'bert/encoder/layer_3/output/dense/bias/adam_v', 'bert/encoder/layer_3/output/dense/kernel', 'bert/encoder/layer_3/output/dense/kernel/adam_m', 'bert/encoder/layer_3/output/dense/kernel/adam_v', 'bert/encoder/layer_4/attention/output/LayerNorm/beta', 'bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_4/attention/output/LayerNorm/gamma', 'bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_4/attention/output/dense/bias', 'bert/encoder/layer_4/attention/output/dense/bias/adam_m', 'bert/encoder/layer_4/attention/output/dense/bias/adam_v', 'bert/encoder/layer_4/attention/output/dense/kernel', 'bert/encoder/layer_4/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_4/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_4/attention/self/key/bias', 'bert/encoder/layer_4/attention/self/key/bias/adam_m', 'bert/encoder/layer_4/attention/self/key/bias/adam_v', 'bert/encoder/layer_4/attention/self/key/kernel', 'bert/encoder/layer_4/attention/self/key/kernel/adam_m', 'bert/encoder/layer_4/attention/self/key/kernel/adam_v', 'bert/encoder/layer_4/attention/self/query/bias', 'bert/encoder/layer_4/attention/self/query/bias/adam_m', 'bert/encoder/layer_4/attention/self/query/bias/adam_v', 'bert/encoder/layer_4/attention/self/query/kernel', 'bert/encoder/layer_4/attention/self/query/kernel/adam_m', 'bert/encoder/layer_4/attention/self/query/kernel/adam_v', 'bert/encoder/layer_4/attention/self/value/bias', 'bert/encoder/layer_4/attention/self/value/bias/adam_m', 'bert/encoder/layer_4/attention/self/value/bias/adam_v', 'bert/encoder/layer_4/attention/self/value/kernel', 'bert/encoder/layer_4/attention/self/value/kernel/adam_m', 'bert/encoder/layer_4/attention/self/value/kernel/adam_v', 'bert/encoder/layer_4/intermediate/dense/bias', 'bert/encoder/layer_4/intermediate/dense/bias/adam_m', 'bert/encoder/layer_4/intermediate/dense/bias/adam_v', 'bert/encoder/layer_4/intermediate/dense/kernel', 'bert/encoder/layer_4/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_4/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_4/output/LayerNorm/beta', 'bert/encoder/layer_4/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_4/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_4/output/LayerNorm/gamma', 'bert/encoder/layer_4/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_4/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_4/output/dense/bias', 'bert/encoder/layer_4/output/dense/bias/adam_m', 'bert/encoder/layer_4/output/dense/bias/adam_v', 'bert/encoder/layer_4/output/dense/kernel', 'bert/encoder/layer_4/output/dense/kernel/adam_m', 'bert/encoder/layer_4/output/dense/kernel/adam_v', 'bert/encoder/layer_5/attention/output/LayerNorm/beta', 'bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_5/attention/output/LayerNorm/gamma', 'bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_5/attention/output/dense/bias', 'bert/encoder/layer_5/attention/output/dense/bias/adam_m', 'bert/encoder/layer_5/attention/output/dense/bias/adam_v', 'bert/encoder/layer_5/attention/output/dense/kernel', 'bert/encoder/layer_5/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_5/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_5/attention/self/key/bias', 'bert/encoder/layer_5/attention/self/key/bias/adam_m', 'bert/encoder/layer_5/attention/self/key/bias/adam_v', 'bert/encoder/layer_5/attention/self/key/kernel', 'bert/encoder/layer_5/attention/self/key/kernel/adam_m', 'bert/encoder/layer_5/attention/self/key/kernel/adam_v', 'bert/encoder/layer_5/attention/self/query/bias', 'bert/encoder/layer_5/attention/self/query/bias/adam_m', 'bert/encoder/layer_5/attention/self/query/bias/adam_v', 'bert/encoder/layer_5/attention/self/query/kernel', 'bert/encoder/layer_5/attention/self/query/kernel/adam_m', 'bert/encoder/layer_5/attention/self/query/kernel/adam_v', 'bert/encoder/layer_5/attention/self/value/bias', 'bert/encoder/layer_5/attention/self/value/bias/adam_m', 'bert/encoder/layer_5/attention/self/value/bias/adam_v', 'bert/encoder/layer_5/attention/self/value/kernel', 'bert/encoder/layer_5/attention/self/value/kernel/adam_m', 'bert/encoder/layer_5/attention/self/value/kernel/adam_v', 'bert/encoder/layer_5/intermediate/dense/bias', 'bert/encoder/layer_5/intermediate/dense/bias/adam_m', 'bert/encoder/layer_5/intermediate/dense/bias/adam_v', 'bert/encoder/layer_5/intermediate/dense/kernel', 'bert/encoder/layer_5/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_5/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_5/output/LayerNorm/beta', 'bert/encoder/layer_5/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_5/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_5/output/LayerNorm/gamma', 'bert/encoder/layer_5/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_5/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_5/output/dense/bias', 'bert/encoder/layer_5/output/dense/bias/adam_m', 'bert/encoder/layer_5/output/dense/bias/adam_v', 'bert/encoder/layer_5/output/dense/kernel', 'bert/encoder/layer_5/output/dense/kernel/adam_m', 'bert/encoder/layer_5/output/dense/kernel/adam_v', 'bert/encoder/layer_6/attention/output/LayerNorm/beta', 'bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_6/attention/output/LayerNorm/gamma', 'bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_6/attention/output/dense/bias', 'bert/encoder/layer_6/attention/output/dense/bias/adam_m', 'bert/encoder/layer_6/attention/output/dense/bias/adam_v', 'bert/encoder/layer_6/attention/output/dense/kernel', 'bert/encoder/layer_6/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_6/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_6/attention/self/key/bias', 'bert/encoder/layer_6/attention/self/key/bias/adam_m', 'bert/encoder/layer_6/attention/self/key/bias/adam_v', 'bert/encoder/layer_6/attention/self/key/kernel', 'bert/encoder/layer_6/attention/self/key/kernel/adam_m', 'bert/encoder/layer_6/attention/self/key/kernel/adam_v', 'bert/encoder/layer_6/attention/self/query/bias', 'bert/encoder/layer_6/attention/self/query/bias/adam_m', 'bert/encoder/layer_6/attention/self/query/bias/adam_v', 'bert/encoder/layer_6/attention/self/query/kernel', 'bert/encoder/layer_6/attention/self/query/kernel/adam_m', 'bert/encoder/layer_6/attention/self/query/kernel/adam_v', 'bert/encoder/layer_6/attention/self/value/bias', 'bert/encoder/layer_6/attention/self/value/bias/adam_m', 'bert/encoder/layer_6/attention/self/value/bias/adam_v', 'bert/encoder/layer_6/attention/self/value/kernel', 'bert/encoder/layer_6/attention/self/value/kernel/adam_m', 'bert/encoder/layer_6/attention/self/value/kernel/adam_v', 'bert/encoder/layer_6/intermediate/dense/bias', 'bert/encoder/layer_6/intermediate/dense/bias/adam_m', 'bert/encoder/layer_6/intermediate/dense/bias/adam_v', 'bert/encoder/layer_6/intermediate/dense/kernel', 'bert/encoder/layer_6/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_6/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_6/output/LayerNorm/beta', 'bert/encoder/layer_6/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_6/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_6/output/LayerNorm/gamma', 'bert/encoder/layer_6/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_6/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_6/output/dense/bias', 'bert/encoder/layer_6/output/dense/bias/adam_m', 'bert/encoder/layer_6/output/dense/bias/adam_v', 'bert/encoder/layer_6/output/dense/kernel', 'bert/encoder/layer_6/output/dense/kernel/adam_m', 'bert/encoder/layer_6/output/dense/kernel/adam_v', 'bert/encoder/layer_7/attention/output/LayerNorm/beta', 'bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_7/attention/output/LayerNorm/gamma', 'bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_7/attention/output/dense/bias', 'bert/encoder/layer_7/attention/output/dense/bias/adam_m', 'bert/encoder/layer_7/attention/output/dense/bias/adam_v', 'bert/encoder/layer_7/attention/output/dense/kernel', 'bert/encoder/layer_7/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_7/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_7/attention/self/key/bias', 'bert/encoder/layer_7/attention/self/key/bias/adam_m', 'bert/encoder/layer_7/attention/self/key/bias/adam_v', 'bert/encoder/layer_7/attention/self/key/kernel', 'bert/encoder/layer_7/attention/self/key/kernel/adam_m', 'bert/encoder/layer_7/attention/self/key/kernel/adam_v', 'bert/encoder/layer_7/attention/self/query/bias', 'bert/encoder/layer_7/attention/self/query/bias/adam_m', 'bert/encoder/layer_7/attention/self/query/bias/adam_v', 'bert/encoder/layer_7/attention/self/query/kernel', 'bert/encoder/layer_7/attention/self/query/kernel/adam_m', 'bert/encoder/layer_7/attention/self/query/kernel/adam_v', 'bert/encoder/layer_7/attention/self/value/bias', 'bert/encoder/layer_7/attention/self/value/bias/adam_m', 'bert/encoder/layer_7/attention/self/value/bias/adam_v', 'bert/encoder/layer_7/attention/self/value/kernel', 'bert/encoder/layer_7/attention/self/value/kernel/adam_m', 'bert/encoder/layer_7/attention/self/value/kernel/adam_v', 'bert/encoder/layer_7/intermediate/dense/bias', 'bert/encoder/layer_7/intermediate/dense/bias/adam_m', 'bert/encoder/layer_7/intermediate/dense/bias/adam_v', 'bert/encoder/layer_7/intermediate/dense/kernel', 'bert/encoder/layer_7/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_7/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_7/output/LayerNorm/beta', 'bert/encoder/layer_7/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_7/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_7/output/LayerNorm/gamma', 'bert/encoder/layer_7/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_7/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_7/output/dense/bias', 'bert/encoder/layer_7/output/dense/bias/adam_m', 'bert/encoder/layer_7/output/dense/bias/adam_v', 'bert/encoder/layer_7/output/dense/kernel', 'bert/encoder/layer_7/output/dense/kernel/adam_m', 'bert/encoder/layer_7/output/dense/kernel/adam_v', 'bert/encoder/layer_8/attention/output/LayerNorm/beta', 'bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_8/attention/output/LayerNorm/gamma', 'bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_8/attention/output/dense/bias', 'bert/encoder/layer_8/attention/output/dense/bias/adam_m', 'bert/encoder/layer_8/attention/output/dense/bias/adam_v', 'bert/encoder/layer_8/attention/output/dense/kernel', 'bert/encoder/layer_8/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_8/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_8/attention/self/key/bias', 'bert/encoder/layer_8/attention/self/key/bias/adam_m', 'bert/encoder/layer_8/attention/self/key/bias/adam_v', 'bert/encoder/layer_8/attention/self/key/kernel', 'bert/encoder/layer_8/attention/self/key/kernel/adam_m', 'bert/encoder/layer_8/attention/self/key/kernel/adam_v', 'bert/encoder/layer_8/attention/self/query/bias', 'bert/encoder/layer_8/attention/self/query/bias/adam_m', 'bert/encoder/layer_8/attention/self/query/bias/adam_v', 'bert/encoder/layer_8/attention/self/query/kernel', 'bert/encoder/layer_8/attention/self/query/kernel/adam_m', 'bert/encoder/layer_8/attention/self/query/kernel/adam_v', 'bert/encoder/layer_8/attention/self/value/bias', 'bert/encoder/layer_8/attention/self/value/bias/adam_m', 'bert/encoder/layer_8/attention/self/value/bias/adam_v', 'bert/encoder/layer_8/attention/self/value/kernel', 'bert/encoder/layer_8/attention/self/value/kernel/adam_m', 'bert/encoder/layer_8/attention/self/value/kernel/adam_v', 'bert/encoder/layer_8/intermediate/dense/bias', 'bert/encoder/layer_8/intermediate/dense/bias/adam_m', 'bert/encoder/layer_8/intermediate/dense/bias/adam_v', 'bert/encoder/layer_8/intermediate/dense/kernel', 'bert/encoder/layer_8/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_8/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_8/output/LayerNorm/beta', 'bert/encoder/layer_8/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_8/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_8/output/LayerNorm/gamma', 'bert/encoder/layer_8/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_8/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_8/output/dense/bias', 'bert/encoder/layer_8/output/dense/bias/adam_m', 'bert/encoder/layer_8/output/dense/bias/adam_v', 'bert/encoder/layer_8/output/dense/kernel', 'bert/encoder/layer_8/output/dense/kernel/adam_m', 'bert/encoder/layer_8/output/dense/kernel/adam_v', 'bert/encoder/layer_9/attention/output/LayerNorm/beta', 'bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_9/attention/output/LayerNorm/gamma', 'bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_9/attention/output/dense/bias', 'bert/encoder/layer_9/attention/output/dense/bias/adam_m', 'bert/encoder/layer_9/attention/output/dense/bias/adam_v', 'bert/encoder/layer_9/attention/output/dense/kernel', 'bert/encoder/layer_9/attention/output/dense/kernel/adam_m', 'bert/encoder/layer_9/attention/output/dense/kernel/adam_v', 'bert/encoder/layer_9/attention/self/key/bias', 'bert/encoder/layer_9/attention/self/key/bias/adam_m', 'bert/encoder/layer_9/attention/self/key/bias/adam_v', 'bert/encoder/layer_9/attention/self/key/kernel', 'bert/encoder/layer_9/attention/self/key/kernel/adam_m', 'bert/encoder/layer_9/attention/self/key/kernel/adam_v', 'bert/encoder/layer_9/attention/self/query/bias', 'bert/encoder/layer_9/attention/self/query/bias/adam_m', 'bert/encoder/layer_9/attention/self/query/bias/adam_v', 'bert/encoder/layer_9/attention/self/query/kernel', 'bert/encoder/layer_9/attention/self/query/kernel/adam_m', 'bert/encoder/layer_9/attention/self/query/kernel/adam_v', 'bert/encoder/layer_9/attention/self/value/bias', 'bert/encoder/layer_9/attention/self/value/bias/adam_m', 'bert/encoder/layer_9/attention/self/value/bias/adam_v', 'bert/encoder/layer_9/attention/self/value/kernel', 'bert/encoder/layer_9/attention/self/value/kernel/adam_m', 'bert/encoder/layer_9/attention/self/value/kernel/adam_v', 'bert/encoder/layer_9/intermediate/dense/bias', 'bert/encoder/layer_9/intermediate/dense/bias/adam_m', 'bert/encoder/layer_9/intermediate/dense/bias/adam_v', 'bert/encoder/layer_9/intermediate/dense/kernel', 'bert/encoder/layer_9/intermediate/dense/kernel/adam_m', 'bert/encoder/layer_9/intermediate/dense/kernel/adam_v', 'bert/encoder/layer_9/output/LayerNorm/beta', 'bert/encoder/layer_9/output/LayerNorm/beta/adam_m', 'bert/encoder/layer_9/output/LayerNorm/beta/adam_v', 'bert/encoder/layer_9/output/LayerNorm/gamma', 'bert/encoder/layer_9/output/LayerNorm/gamma/adam_m', 'bert/encoder/layer_9/output/LayerNorm/gamma/adam_v', 'bert/encoder/layer_9/output/dense/bias', 'bert/encoder/layer_9/output/dense/bias/adam_m', 'bert/encoder/layer_9/output/dense/bias/adam_v', 'bert/encoder/layer_9/output/dense/kernel', 'bert/encoder/layer_9/output/dense/kernel/adam_m', 'bert/encoder/layer_9/output/dense/kernel/adam_v', 'bert/pooler/dense/bias', 'bert/pooler/dense/bias/adam_m', 'bert/pooler/dense/bias/adam_v', 'bert/pooler/dense/kernel', 'bert/pooler/dense/kernel/adam_m', 'bert/pooler/dense/kernel/adam_v', 'output_bias', 'output_bias/adam_m', 'output_bias/adam_v', 'output_weights', 'output_weights/adam_m', 'output_weights/adam_v']\n",
            "model bert\n",
            "model embeddings\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model embeddings\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model embeddings\n",
            "model position_embeddings\n",
            "model bert\n",
            "model embeddings\n",
            "model token_type_embeddings\n",
            "model bert\n",
            "model embeddings\n",
            "model word_embeddings\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_0\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_1\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_10\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_11\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_12\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_13\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_14\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_15\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_16\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_17\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_18\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_19\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_2\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_20\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_21\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_22\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_23\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_3\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_4\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_5\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_6\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_7\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_8\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model self\n",
            "model key\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model self\n",
            "model query\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model attention\n",
            "model self\n",
            "model value\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model intermediate\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model intermediate\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model output\n",
            "model LayerNorm\n",
            "model beta\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model output\n",
            "model LayerNorm\n",
            "model gamma\n",
            "['gamma']\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model output\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model encoder\n",
            "model layer_9\n",
            "model output\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model bert\n",
            "model pooler\n",
            "model dense\n",
            "model bias\n",
            "model bert\n",
            "model pooler\n",
            "model dense\n",
            "model kernel\n",
            "['kernel']\n",
            "model classifier\n",
            "model output_bias\n",
            "model classifier\n",
            "model output_weights\n",
            "['output_weights']\n",
            "Save PyTorch model to pytorch_output_temp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FZtqz7s8zi",
        "colab_type": "text"
      },
      "source": [
        "### Upload converted checkpoint and test inference\n",
        "If everything goes smoothly we should be able to upload weights and use the converted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J3J-hQ2kgmS",
        "colab_type": "code",
        "outputId": "c260ea53-a41d-4a87-b551-f8723d24aa8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('biobert_large/vocab_cased_pubmed_pmc_30k.txt')\n",
        "model2.eval()\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "outputs = model2(input_ids)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16wiahNtk6DN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "18664805-81e2-4c22-e293-6f023e60ccd2"
      },
      "source": [
        "outputs = model2(input_ids)\n",
        "outputs"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.2204,  0.6557]], grad_fn=<AddmmBackward>),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwWXhCMZk68b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(tokenizer.encode(\"All our results indicate that the presence of the @GENE$ genotype (++) in patients with structural @DISEASE$, severe left ventricular dysfunction and malignant ventricular arrhythmias increases the risk for these patients of hemodynamic collapse during these arrhythmias\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRvIr2tBlcKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1255d1fd-66e7-433e-be78-27232a9fd1e6"
      },
      "source": [
        "outputs = model2(input_ids.unsqueeze(0))\n",
        "outputs"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.4003,  0.4932]], grad_fn=<AddmmBackward>),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2VYSxPfmG-8",
        "colab_type": "code",
        "outputId": "1d2e1296-e77b-4725-f2c7-2c72bcdf88db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "values, indices = torch.max(outputs[0], 1, keepdim=False)\n",
        "indices\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g16-IRcwzHhI",
        "colab_type": "text"
      },
      "source": [
        "**Lets refactor this into something nicer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8woK7_wmLqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertConfig, BertForSequenceClassification, BertForPreTraining\n",
        "from transformers import BertTokenizer\n",
        "class InferSequenceClassifier(object):\n",
        "  def __init__(self, pytorch_model_path, token_path, add_special_tokens=False):\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(token_path)\n",
        "    self.model = BertForSequenceClassification.from_pretrained(pytorch_model_path)\n",
        "    self.add_special_tokens = add_special_tokens\n",
        "\n",
        "  def make_prediction(self, text):\n",
        "    input_ids = torch.tensor(self.tokenizer.encode(text, add_special_tokens=self.add_special_tokens))\n",
        "    outputs = self.model(input_ids.unsqueeze(0))\n",
        "    print(outputs)\n",
        "    values, indices = torch.max(outputs[0], 1, keepdim=False)\n",
        "    return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdx7Fzx7uO_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp biobert_large/vocab_cased_pubmed_pmc_30k.txt pytorch_output_temp/vocab.txt\n",
        "!cp biobert_large/bert_config_bio_58k_large.json pytorch_output_temp/config.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdMtWmSCo-jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_infer = InferSequenceClassifier(\"pytorch_output_temp\", \"pytorch_output_temp\", True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWF3Du8Kurdh",
        "colab_type": "code",
        "outputId": "8ca3c094-a5b3-44e4-b983-2127c29d1a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "seq_infer.make_prediction(\"@GENE$ influences brain beta-@DISEASE$ load, cerebrospinal fluid levels of beta-amyloid peptides and phosphorylated tau, and the genetic risk of late-onset sporadic AD.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[-0.3223,  0.5159]], grad_fn=<AddmmBackward>),)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6V_dHwOz67w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6456d84e-5097-4ae6-e986-a7891bacaba3"
      },
      "source": [
        "seq_infer.make_prediction(\"All our results indicate that the presence of the @GENE$ genotype (++) in patients with structural @DISEASE$, severe left ventricular dysfunction and malignant ventricular arrhythmias increases the risk for these patients of hemodynamic collapse during these arrhythmias\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[-0.4003,  0.4932]], grad_fn=<AddmmBackward>),)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXkt1mhY3NME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b5747842-4b55-4c20-badf-bfa93e2093bf"
      },
      "source": [
        "seq_infer.make_prediction(\"Functional studies to unravel the biological significance of this region in regulating @GENE$ production is clearly indicated, which may lead to new strategies to modify the disease course of severe @DISEASE$.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[-0.3648,  0.4784]], grad_fn=<AddmmBackward>),)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jVvZR6uuQpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -r pytorch_output_temp gs://coronavirusqa/re_convert"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}