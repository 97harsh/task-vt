{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets extraction against articles' URL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vrw3MbzFlr_",
        "colab_type": "text"
      },
      "source": [
        "# By @Christine Yang\n",
        "# Extraction of tweets against the 'URL' of scientific papers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZoUA_jH0_My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/Users/christine/anaconda3/bin/python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os, re, pandas as pd, time\n",
        "import tweepy as tw\n",
        "from tweepy.auth import OAuthHandler\n",
        "\n",
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token_key = ''\n",
        "access_token_secret = ''\n",
        "\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token_key, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# SEARCH TWEETS\n",
        "def search_article(query):\n",
        "    query = '\"'+query+'\"'\n",
        "    try:\n",
        "        # get 999 tweets since Oct. 1 2019, untruncated\n",
        "        tweets = tw.Cursor(api.search, q=query, since=\"2019-10-01\", tweet_mode='extended').items(999)\n",
        "        tweets = [tweet.full_text for tweet in tweets]\n",
        "        print(len(tweets), query) # progress\n",
        "        return tweets\n",
        "    except:\n",
        "        print('ERROR', query)\n",
        "        return []\n",
        "\n",
        "# GET LIST OF ARTICLES\n",
        "df = pd.read_csv('articles_vaccine_covid2019.csv', encoding='utf-8-sig')\n",
        "links = df['URL for article']\n",
        "titles = df['Article name']\n",
        "titles = [re.sub(r' *\\[.*\\] *', '', t) for t in titles] # remove stuff in brackets\n",
        "articles = [(titles[i], links[i]) for i in range(len(links))] # [(title, link), ...]\n",
        "\n",
        "# SEARCH FOR ARTICLE MENTIONS ON TWITTER\n",
        "tweets, article_titles, article_links, found_by = [], [], [], []\n",
        "i = 0\n",
        "for title, link in articles[i:]:\n",
        "    # search for article title\n",
        "    t = search_article(title)\n",
        "    tweets.extend(t)\n",
        "    article_titles.extend([title]*len(t))\n",
        "    article_links.extend([link]*len(t))\n",
        "    found_by.extend(['article_title']*len(t))\n",
        "    \n",
        "    # search for article link\n",
        "    t = search_article('url:'+link)\n",
        "    tweets.extend(t)\n",
        "    article_titles.extend([title]*len(t))\n",
        "    article_links.extend([link]*len(t))\n",
        "    found_by.extend(['article_link']*len(t))\n",
        "    # checkpoint save\n",
        "    if i % 10 == 0:\n",
        "        df = pd.DataFrame({'tweet':tweets, 'article_title':article_titles, 'article_link':article_links, 'found_by':found_by})\n",
        "        df = df.drop_duplicates()\n",
        "        df.to_csv('result.csv', index=False, encoding='utf-8-sig')\n",
        "    print(i) # progress\n",
        "    i += 1\n",
        "    time.sleep(10) # wait a bit\n",
        "\n",
        "# SAVE RESULTS TO CSV\n",
        "print(len(tweets))\n",
        "df = pd.DataFrame({'tweet':tweets, 'article_title':article_titles, 'article_link':article_links, 'found_by':found_by})\n",
        "df = df.drop_duplicates()\n",
        "df.to_csv('VT_tweets.csv', index=False, encoding='utf-8-sig')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}